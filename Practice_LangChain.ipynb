{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "bUejK7rt2WP5",
        "outputId": "debccf31-7f42-47be-e855-75b06772434b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your Google AI API key: ··········\n"
          ]
        }
      ],
      "source": [
        "%pip -q install -U langchain langchain-community langchain-google-genai faiss-cpu tiktoken\n",
        "\n",
        "import os, getpass\n",
        "\n",
        "if \" \" not in os.environ:\n",
        "    os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter your Google AI API key: \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "81S0xHBr2_bX",
        "outputId": "5a7deff3-6d44-4370-c0f0-5f0f81cee57a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hello from LangChain!\n"
          ]
        }
      ],
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\")  # fast, inexpensive model\n",
        "resp = llm.invoke(\"Say hello from LangChain in one short sentence.\")\n",
        "print(resp.content)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "frUMf9iA4Gc5",
        "outputId": "6918d7ee-1aaf-47b0-d3eb-562bd8d1a0a4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Renewable energy sources offer a multitude of benefits compared to fossil fuels:\n",
            "\n",
            "**Environmental Benefits:**\n",
            "\n",
            "* **Reduced Greenhouse Gas Emissions:** This is arguably the most significant benefit.  Renewable energy sources produce little to no greenhouse gases during operation, significantly reducing our carbon footprint and mitigating climate change.\n",
            "* **Improved Air Quality:**  Burning fossil fuels releases harmful pollutants like particulate matter, sulfur dioxide, and nitrogen oxides, which contribute to respiratory illnesses and other health problems. Renewable energy sources drastically reduce these pollutants, leading to cleaner air.\n",
            "* **Reduced Water Pollution:** Fossil fuel extraction and processing often contaminate water sources. Renewable energy technologies generally have a much lower impact on water quality.\n",
            "* **Protection of Ecosystems:**  Renewable energy sources require less land disruption than fossil fuel extraction, helping to protect natural habitats and biodiversity.  While some land use is required for renewable energy projects, it's often less extensive and can sometimes coexist with other land uses (e.g., solar farms on grazing land).\n",
            "* **Reduced Waste:**  Renewable energy technologies generally produce less waste than fossil fuel power plants.\n",
            "\n",
            "\n",
            "**Economic Benefits:**\n",
            "\n",
            "* **Energy Independence:**  Renewable energy reduces reliance on imported fossil fuels, enhancing national energy security and reducing vulnerability to price volatility in global energy markets.\n",
            "{'input_tokens': 8, 'output_tokens': 256, 'total_tokens': 264, 'input_token_details': {'cache_read': 0}}\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-1.5-flash\",\n",
        "    temperature=0.7,\n",
        "    max_tokens=256,\n",
        ")\n",
        "\n",
        "# Simple usage\n",
        "response = llm.invoke(\"What are the benefits of renewable energy?\")\n",
        "print(response.content)        # Text output\n",
        "print(response.usage_metadata) # Optional: token usage details"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hdS1iplb4PR5",
        "outputId": "50db2fca-345e-4323-880d-46cd4a204052"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Imagine a light switch. It can be either ON or OFF, right?  That's how regular computers work – they use bits, which are either 0 or 1.\n",
            "\n",
            "Quantum computers are different. They use **qubits**.  A qubit can be 0, 1, or *both at the same time*! This \"both at the same time\" is called **superposition**.  It's like the light switch being both ON and OFF simultaneously – weird, but true.\n",
            "\n",
            "Another quantum trick is **entanglement**. Imagine two qubits linked together magically.  If you measure one and it's 0, you instantly know the other is 1, no matter how far apart they are!\n",
            "\n",
            "Because of superposition and entanglement, quantum computers can explore many possibilities at once.  Think of it like searching a maze: a regular computer tries each path one by one, while a quantum computer explores all paths simultaneously.  This makes them potentially much faster for certain types of problems.\n",
            "\n",
            "However, quantum computers aren't meant to replace regular computers. They're good at specific tasks, like:\n",
            "\n",
            "* **Drug discovery:** Simulating molecules to design new medicines.\n",
            "* **Materials science:** Designing new materials with specific properties\n"
          ]
        }
      ],
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_core.messages import HumanMessage, SystemMessage\n",
        "\n",
        "chat_model = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-1.5-flash\",\n",
        "    temperature=0.3,\n",
        "    max_tokens=256,\n",
        ")\n",
        "\n",
        "messages = [\n",
        "    SystemMessage(content=\"You are a helpful AI assistant.\"),\n",
        "    HumanMessage(content=\"Explain quantum computing in simple terms.\"),\n",
        "]\n",
        "\n",
        "response = chat_model.invoke(messages)\n",
        "\n",
        "print(response.content)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "jZVdtot94pR7",
        "outputId": "09b7f124-52ef-453d-d4b3-158382040301"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Embedding dimension: 3072\n",
            "First 5 values: [-0.011009139940142632, 0.003710047109052539, 0.007905430160462856, -0.07840708643198013, -0.012440552935004234]\n"
          ]
        }
      ],
      "source": [
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "\n",
        "embeddings = GoogleGenerativeAIEmbeddings(\n",
        "    model=\"gemini-embedding-001\"\n",
        ")\n",
        "\n",
        "text = \"LangChain is a powerful framework for LLM applications.\"\n",
        "embedding_vector = embeddings.embed_query(text)\n",
        "\n",
        "print(f\"Embedding dimension: {len(embedding_vector)}\")\n",
        "print(f\"First 5 values: {embedding_vector[:5]}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "SQAviZGF7dt1",
        "outputId": "1e97ea40-3722-436a-a29c-a55d713d6073"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "----- Formatted Prompt -----\n",
            "\n",
            "You are an expert financial planning consultant.\n",
            "Please provide advice on: retirement savings strategies\n",
            "Consider the following context: for a 30-year-old software engineer earning $120k annually\n",
            "Your advice:\n",
            "\n",
            "\n",
            "----- Model Response -----\n",
            "## Retirement Savings Strategies for a 30-Year-Old Software Engineer Earning $120k Annually\n",
            "\n",
            "Congratulations on your successful career as a software engineer!  At 30, you're in an excellent position to build a robust retirement plan.  However, simply saving is insufficient; a strategic approach is crucial.  Here's a comprehensive plan tailored to your situation:\n",
            "\n",
            "**I. Maximizing Employer-Sponsored Retirement Plans:**\n",
            "\n",
            "* **401(k) or Similar Plan:**  Your highest priority should be maximizing contributions to your employer-sponsored retirement plan.  If your employer offers a matching contribution (e.g., 50% up to 6% of your salary), contribute at least that much – this is essentially free money.  Aim to contribute the maximum allowable amount ($23,000 in 2023, plus an additional $7,500 catch-up contribution if you're 50 or older).  Consider increasing your contributions gradually over time to ease the adjustment.\n",
            "\n",
            "* **Understand Investment Options:** Don't just passively accept the default investment option.  Research the available funds within your plan.  A diversified portfolio of low-cost index funds (tracking the S&P 500, total stock market, and potentially bonds) is generally recommended for long-term growth.  Consider your risk tolerance and time horizon when making investment choices.  If you'\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.prompts import PromptTemplate\n",
        "\n",
        "template = \"\"\"\n",
        "You are an expert {expertise} consultant.\n",
        "Please provide advice on: {query}\n",
        "Consider the following context: {context}\n",
        "Your advice:\n",
        "\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"expertise\", \"query\", \"context\"],\n",
        "    template=template,\n",
        ")\n",
        "\n",
        "formatted_prompt = prompt.format(\n",
        "    expertise=\"financial planning\",\n",
        "    query=\"retirement savings strategies\",\n",
        "    context=\"for a 30-year-old software engineer earning $120k annually\",\n",
        ")\n",
        "print(\"----- Formatted Prompt -----\")\n",
        "print(formatted_prompt)\n",
        "\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-1.5-flash\",\n",
        "    temperature=0.3,\n",
        "    max_tokens=300,\n",
        ")\n",
        "\n",
        "response = llm.invoke(formatted_prompt)\n",
        "print(\"\\n----- Model Response -----\")\n",
        "print(response.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "Mf3Vsrb09xbr",
        "outputId": "0751a291-09a1-46e6-c290-ce50ad33668c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SystemMessage: You are a data scientist who specializes in machine learning model optimization.\n",
            "HumanMessage: How can I improve the performance of my neural network?\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.prompts.chat import (\n",
        "    SystemMessagePromptTemplate,\n",
        "    HumanMessagePromptTemplate,\n",
        ")\n",
        "\n",
        "# Create chat prompt template\n",
        "system_template = \"You are a {role} who {specialty}.\"\n",
        "system_message_prompt = SystemMessagePromptTemplate.from_template(system_template)\n",
        "\n",
        "human_template = \"{request}\"\n",
        "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
        "\n",
        "chat_prompt = ChatPromptTemplate.from_messages([\n",
        "    system_message_prompt,\n",
        "    human_message_prompt,\n",
        "])\n",
        "\n",
        "# Format the messages\n",
        "messages = chat_prompt.format_messages(\n",
        "    role=\"data scientist\",\n",
        "    specialty=\"specializes in machine learning model optimization\",\n",
        "    request=\"How can I improve the performance of my neural network?\"\n",
        ")\n",
        "\n",
        "for message in messages:\n",
        "    print(f\"{message.__class__.__name__}: {message.content}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "CV2VFzomnn7x",
        "outputId": "0b78e4c0-695f-407c-a64e-7a3ce7922c15"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You are a Python programming expert. Provide helpful library recommendations:\n",
            "\n",
            "Input: I need to analyze sales data\n",
            "Output: I recommend using pandas for data manipulation, matplotlib for visualization, and seaborn for statistical plots.\n",
            "\n",
            "Input: How do I build a web scraper?\n",
            "Output: For web scraping, use requests for HTTP, BeautifulSoup for HTML parsing, and optionally Selenium for dynamic pages.\n",
            "\n",
            "Input: I want to build a REST API for my project\n",
            "Output:\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.prompts import PromptTemplate, FewShotPromptTemplate\n",
        "\n",
        "# Define examples\n",
        "examples = [\n",
        "    {\n",
        "        \"input\": \"I need to analyze sales data\",\n",
        "        \"output\": \"I recommend using pandas for data manipulation, matplotlib for visualization, and seaborn for statistical plots.\"\n",
        "    },\n",
        "    {\n",
        "        \"input\": \"How do I build a web scraper?\",\n",
        "        \"output\": \"For web scraping, use requests for HTTP, BeautifulSoup for HTML parsing, and optionally Selenium for dynamic pages.\"\n",
        "    }\n",
        "]\n",
        "\n",
        "# Create example template\n",
        "example_template = \"\"\"\n",
        "Input: {input}\n",
        "Output: {output}\n",
        "\"\"\".strip()\n",
        "\n",
        "example_prompt = PromptTemplate(\n",
        "    input_variables=[\"input\", \"output\"],\n",
        "    template=example_template,\n",
        ")\n",
        "\n",
        "# Create few-shot prompt\n",
        "few_shot_prompt = FewShotPromptTemplate(\n",
        "    examples=examples,\n",
        "    example_prompt=example_prompt,\n",
        "    prefix=\"You are a Python programming expert. Provide helpful library recommendations:\",\n",
        "    suffix=\"Input: {query}\\nOutput:\",\n",
        "    input_variables=[\"query\"],\n",
        "    example_separator=\"\\n\\n\",\n",
        ")\n",
        "\n",
        "# Use the prompt\n",
        "formatted = few_shot_prompt.format(query=\"I want to build a REST API for my project\")\n",
        "print(formatted)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "LzSYzyNnn52-",
        "outputId": "3974b7a4-d11e-4709-8eed-3070b2e2a386"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Artificial intelligence (AI) is a broad field encompassing the development of computer systems capable of performing tasks that typically require human intelligence.  These tasks include learning, reasoning, problem-solving, perception, and natural language understanding.  AI systems achieve this through various techniques, including machine learning (where systems learn from data without explicit programming) and deep learning (a subset of machine learning using artificial neural networks with multiple layers).  AI is rapidly transforming many industries, from healthcare and finance to transportation and entertainment, offering both immense potential and significant ethical considerations.\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "# Create components\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"topic\"],\n",
        "    template=\"Write a brief summary about {topic}:\"\n",
        ")\n",
        "\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-1.5-flash\",\n",
        "    temperature=0.7,\n",
        ")\n",
        "\n",
        "# Create chain (prompt -> model -> string)\n",
        "chain = prompt | llm | StrOutputParser()\n",
        "\n",
        "# Run chain\n",
        "result = chain.invoke({\"topic\": \"artificial intelligence\"})\n",
        "print(result)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "9A2QtEv4owaW",
        "outputId": "8f6ac4ad-3255-4f4d-c6fb-2b3abf42b005"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.5/310.5 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hLoaded 2 pages\n",
            "Page 1: Here’s a demo setup diagram for your Smart Energy Optimization System:\n",
            "🔹 Demo Setup Diagram\n",
            "        ┌───────────────────────────┐\n",
            "        │        Sen...\n",
            "Page 2: This diagram is simple, clear , and examiner-friendly. ✅\n",
            "2...\n"
          ]
        }
      ],
      "source": [
        "%pip -q install -U langchain-community pypdf\n",
        "\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "\n",
        "# Load a PDF file\n",
        "loader = PyPDFLoader(\"Demo Setup Diagram.pdf\")\n",
        "pages = loader.load()\n",
        "\n",
        "print(f\"Loaded {len(pages)} pages\")\n",
        "for i, page in enumerate(pages[:3]):  # Show first 3 pages\n",
        "    print(f\"Page {i + 1}: {page.page_content[:150]}...\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "OKCvyJ33pOrl",
        "outputId": "eb2f0b8e-d045-4c1c-fc91-fd2f167121e7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: ddgs in /usr/local/lib/python3.12/dist-packages (9.5.5)\n",
            "Requirement already satisfied: click>=8.1.8 in /usr/local/lib/python3.12/dist-packages (from ddgs) (8.2.1)\n",
            "Requirement already satisfied: primp>=0.15.0 in /usr/local/lib/python3.12/dist-packages (from ddgs) (0.15.0)\n",
            "Requirement already satisfied: lxml>=6.0.0 in /usr/local/lib/python3.12/dist-packages (from ddgs) (6.0.1)\n",
            "==================================================\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m\n",
            "Invoking: `calculator` with `{'expression': '15 * 23 + 47'}`\n",
            "\n",
            "\n",
            "\u001b[0m\u001b[36;1m\u001b[1;3mThe result is: 392\u001b[0m\u001b[32;1m\u001b[1;3mThe answer is 392.\n",
            "\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "Query: What is 15 * 23 + 47?\n",
            "Answer: The answer is 392.\n",
            "\n",
            "\n",
            "Intermediate Steps:\n",
            "- Action: calculator\n",
            "  Input:  {'expression': '15 * 23 + 47'}\n",
            "  Output: The result is: 392\n",
            "\n",
            "==================================================\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m\n",
            "Invoking: `weather_info` with `{'city': 'New York'}`\n",
            "\n",
            "\n",
            "\u001b[0m\u001b[33;1m\u001b[1;3mThe weather in New York is sunny with a temperature of 22°C\u001b[0m\u001b[32;1m\u001b[1;3mThe weather in New York is sunny with a temperature of 22°C.\n",
            "\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "Query: What's the weather like in New York?\n",
            "Answer: The weather in New York is sunny with a temperature of 22°C.\n",
            "\n",
            "\n",
            "Intermediate Steps:\n",
            "- Action: weather_info\n",
            "  Input:  {'city': 'New York'}\n",
            "  Output: The weather in New York is sunny with a temperature of 22°C\n",
            "\n",
            "==================================================\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m\n",
            "Invoking: `web_search` with `{'query': 'recent developments in artificial intelligence'}`\n",
            "\n",
            "\n",
            "\u001b[0m\u001b[38;5;200m\u001b[1;3mAI and machine learning (ML) enable us to analyze massive amounts of data and discover complex patterns and relationships that would be impossible for humans to find. AI and ML can also generate new hypotheses, design experiments, and test theories, augmenting human intelligence and creativity. By 2025, AI and ML will be widely used in scientific r... See full list on indatalabs.com By 2025, AI will be a major factor in international relations, influencing diplomacy, trade, warfare, and human rights. Governments will invest heavily in AI research and innovation, as well as in building alliances and partnerships with other countries and organizations. AI will also pose new challenges for global governance, requiring new norms a... See full list on indatalabs.com By 2025, AI will be integrated into many aspects of our daily lives, from education to entertainment, from health to finance. AI will enable us to do more with less time and effort, improving our productivity and well-being. As we can see, the latest development in the field of artificial intelligence is not only about replacing humans but also abo... See full list on indatalabs.com AI is expected to generate significant economic value in the coming years, creating new markets, products, services, and jobs. However, AI will also disrupt existing industries and occupations, requiring workers to adapt their skills and roles. AI will also raise ethical and social issues, such as privacy, fairness, accountability, and trust. By 20... See full list on indatalabs.com AI will also help improve learning outcomes, customize curricula, provide feedback, and enhance accessibility. AI in learningwill also create more engaging and immersive experiences, such as virtual reality, augmented reality, gaming, and art. AI will also facilitate communication, collaboration, and empathy across cultures and languages. Virtual r... See full list on indatalabs.com Read the latest on artificial intelligence and machine learning tech, the companies that are building them, and the ethical issues AI raises today. Apr 29, 2025 · Explore the latest artificial intelligence news with Reuters - from AI breakthroughs and technology trends to regulation, ethics, business and global impact. What are the latest artificial intelligence developments? Here are some of the latest artificial intelligence developments that you should know about. Many robotics startups are now embracing AI to develop and automate new AI systems. Firms like Olis Robotics have been driving innovation through the use of plug-and-play controls fitted with AI-powered software. How AI is transforming the world of business? From manufacturing and robotics to pharmaceuticals, supply chain management, and everyday AI software automation, it’s happening everywhere. Artificial intelligence history and latest developments are rapidly streamlining the world of business with its automation and smart decision-making capabilities . Is artificial intelligence disrupting the world? It’s already disrupting many of these industries and is expected to shake up many more in the years to come. Public and private companies alike are quickly coming to realize that, given the correct guidance and structure, artificial intelligence recent developments can produce massive and positive changes in virtually every aspect of human life. How is artificial intelligence transforming the world? Artificial intelligence is rapidly creating exciting new opportunities across a wide variety of industries and sectors. It’s already disrupting many of these industries and is expected to shake up many more in the years to come. How artificial intelligence is transforming the future of Transportation? Recent developments in AI have allowed for the augmentation of AI training by computer-generated X-rays to create a larger database. Through this database, neural networks are trained to identify rare conditions. Self-driving cars are being widely hailed as the latest development in artificial intelligence and the future of transportation. What will AI and robotics be like in 2023- 2025? Here are some predictions for the state of AI and robotics in 2023- 2025, based on the latest AI developments . AI and machine learning (ML) enable us to analyze massive amounts of data and discover complex patterns and relationships that would be impossible for humans to find. May 21, 2025 · We’ve narrowed things down to a list of 10: 5 developments that have driven the first half of the year, and 5 more that we expect to play a major role in the months to come. ... in my article on AI timelines , many AI experts believe that there is a real chance that human-level artificial intelligence will be developed within ... Google says its latest AI model called Genie 3 takes us closer to artificial general intelligence . ... Smarter AI is driving up costs for developers ... Recent Advancements in AI ( artificial intelligence ) technology Artificial intelligence (AI) is a fundamental series of technology where devices such ... ... in pop culture and science fiction, artificial ... https://www.allthescience.org/what-are-some- recent -advances- in - artificial - intelligence .htm\u001b[0m\u001b[32;1m\u001b[1;3mRecent developments in artificial intelligence include advancements in AI training using computer-generated X-rays to create larger databases for neural network training, leading to better identification of rare conditions.  Self-driving cars represent another significant development in the field.  There's also progress in AI's integration into various sectors, from manufacturing and robotics to pharmaceuticals and supply chain management.  New AI models, like Google's Genie 3, are pushing the boundaries of artificial general intelligence.  However, these advancements also raise ethical and social issues that need to be addressed.\n",
            "\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "Query: Search for recent developments in artificial intelligence\n",
            "Answer: Recent developments in artificial intelligence include advancements in AI training using computer-generated X-rays to create larger databases for neural network training, leading to better identification of rare conditions.  Self-driving cars represent another significant development in the field.  There's also progress in AI's integration into various sectors, from manufacturing and robotics to pharmaceuticals and supply chain management.  New AI models, like Google's Genie 3, are pushing the boundaries of artificial general intelligence.  However, these advancements also raise ethical and social issues that need to be addressed.\n",
            "\n",
            "\n",
            "Intermediate Steps:\n",
            "- Action: web_search\n",
            "  Input:  {'query': 'recent developments in artificial intelligence'}\n",
            "  Output: AI and machine learning (ML) enable us to analyze massive amounts of data and discover complex patterns and relationships that would be impossible for humans to find. AI and ML can also generate new hypotheses, design experiments, and test theories, augmenting human intelligence and creativity. By 2025, AI and ML will be widely used in scientific r... See full list on indatalabs.com By 2025, AI will be a major factor in international relations, influencing diplomacy, trade, warfare, and human rights. Governments will invest heavily in AI research and innovation, as well as in building alliances and partnerships with other countries and organizations. AI will also pose new challenges for global governance, requiring new norms a... See full list on indatalabs.com By 2025, AI will be integrated into many aspects of our daily lives, from education to entertainment, from health to finance. AI will enable us to do more with less time and effort, improving our productivity and well-being. As we can see, the latest development in the field of artificial intelligence is not only about replacing humans but also abo... See full list on indatalabs.com AI is expected to generate significant economic value in the coming years, creating new markets, products, services, and jobs. However, AI will also disrupt existing industries and occupations, requiring workers to adapt their skills and roles. AI will also raise ethical and social issues, such as privacy, fairness, accountability, and trust. By 20... See full list on indatalabs.com AI will also help improve learning outcomes, customize curricula, provide feedback, and enhance accessibility. AI in learningwill also create more engaging and immersive experiences, such as virtual reality, augmented reality, gaming, and art. AI will also facilitate communication, collaboration, and empathy across cultures and languages. Virtual r... See full list on indatalabs.com Read the latest on artificial intelligence and machine learning tech, the companies that are building them, and the ethical issues AI raises today. Apr 29, 2025 · Explore the latest artificial intelligence news with Reuters - from AI breakthroughs and technology trends to regulation, ethics, business and global impact. What are the latest artificial intelligence developments? Here are some of the latest artificial intelligence developments that you should know about. Many robotics startups are now embracing AI to develop and automate new AI systems. Firms like Olis Robotics have been driving innovation through the use of plug-and-play controls fitted with AI-powered software. How AI is transforming the world of business? From manufacturing and robotics to pharmaceuticals, supply chain management, and everyday AI software automation, it’s happening everywhere. Artificial intelligence history and latest developments are rapidly streamlining the world of business with its automation and smart decision-making capabilities . Is artificial intelligence disrupting the world? It’s already disrupting many of these industries and is expected to shake up many more in the years to come. Public and private companies alike are quickly coming to realize that, given the correct guidance and structure, artificial intelligence recent developments can produce massive and positive changes in virtually every aspect of human life. How is artificial intelligence transforming the world? Artificial intelligence is rapidly creating exciting new opportunities across a wide variety of industries and sectors. It’s already disrupting many of these industries and is expected to shake up many more in the years to come. How artificial intelligence is transforming the future of Transportation? Recent developments in AI have allowed for the augmentation of AI training by computer-generated X-rays to create a larger database. Through this database, neural networks are trained to identify rare conditions. Self-driving cars are being widely hailed as the latest development in artificial intelligence and the future of transportation. What will AI and robotics be like in 2023- 2025? Here are some predictions for the state of AI and robotics in 2023- 2025, based on the latest AI developments . AI and machine learning (ML) enable us to analyze massive amounts of data and discover complex patterns and relationships that would be impossible for humans to find. May 21, 2025 · We’ve narrowed things down to a list of 10: 5 developments that have driven the first half of the year, and 5 more that we expect to play a major role in the months to come. ... in my article on AI timelines , many AI experts believe that there is a real chance that human-level artificial intelligence will be developed within ... Google says its latest AI model called Genie 3 takes us closer to artificial general intelligence . ... Smarter AI is driving up costs for developers ... Recent Advancements in AI ( artificial intelligence ) technology Artificial intelligence (AI) is a fundamental series of technology where devices such ... ... in pop culture and science fiction, artificial ... https://www.allthescience.org/what-are-some- recent -advances- in - artificial - intelligence .htm\n",
            "\n"
          ]
        }
      ],
      "source": [
        "%pip -q install -U langchain langchain-community langchain-google-genai duckduckgo-search\n",
        "%pip install -U ddgs\n",
        "\n",
        "import math\n",
        "\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain.agents import AgentExecutor, create_openai_functions_agent\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain_core.tools import tool\n",
        "from langchain_community.tools import DuckDuckGoSearchRun\n",
        "\n",
        "# Initialize LLM (Gemini 1.5 Flash)\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\", temperature=0)\n",
        "\n",
        "# Define custom tools\n",
        "@tool\n",
        "def calculator(expression: str) -> str:\n",
        "    \"\"\"Calculate mathematical expressions. Use this for any math calculations.\"\"\"\n",
        "    try:\n",
        "        # Caution: eval is for demo purposes only.\n",
        "        result = eval(expression, {\"__builtins__\": {}}, {\"math\": math})\n",
        "        return f\"The result is: {result}\"\n",
        "    except Exception as e:\n",
        "        return f\"Error: {str(e)}\"\n",
        "\n",
        "@tool\n",
        "def weather_info(city: str) -> str:\n",
        "    \"\"\"Get current weather information for a city (mock).\"\"\"\n",
        "    # This is a mock implementation - replace with a real weather API\n",
        "    return f\"The weather in {city} is sunny with a temperature of 22°C\"\n",
        "\n",
        "@tool\n",
        "def web_search(query: str) -> str:\n",
        "    \"\"\"Search the web for information.\"\"\"\n",
        "    search = DuckDuckGoSearchRun()\n",
        "    return search.run(query)\n",
        "\n",
        "# Create tools list\n",
        "tools = [calculator, weather_info, web_search]\n",
        "\n",
        "# Create prompt template (must include agent_scratchpad placeholder)\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are a helpful assistant that can use tools to answer questions.\"),\n",
        "    MessagesPlaceholder(\"chat_history\", optional=True),\n",
        "    (\"human\", \"{input}\"),\n",
        "    MessagesPlaceholder(\"agent_scratchpad\"),\n",
        "])\n",
        "\n",
        "# Create agent using OpenAI-functions style tool calling\n",
        "agent = create_openai_functions_agent(llm, tools, prompt)\n",
        "\n",
        "# Create agent executor\n",
        "agent_executor = AgentExecutor(\n",
        "    agent=agent,\n",
        "    tools=tools,\n",
        "    verbose=True,\n",
        "    return_intermediate_steps=True,\n",
        ")\n",
        "\n",
        "# Use the agent\n",
        "def run_agent_query(query: str):\n",
        "    \"\"\"Run a query through the agent.\"\"\"\n",
        "    result = agent_executor.invoke({\"input\": query})\n",
        "    print(f\"Query: {query}\")\n",
        "    print(f\"Answer: {result['output']}\\n\")\n",
        "    print(\"Intermediate Steps:\")\n",
        "    for action, observation in result.get(\"intermediate_steps\", []):\n",
        "        print(f\"- Action: {action.tool}\")\n",
        "        print(f\"  Input:  {action.tool_input}\")\n",
        "        print(f\"  Output: {observation}\")\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    queries = [\n",
        "        \"What is 15 * 23 + 47?\",\n",
        "        \"What's the weather like in New York?\",\n",
        "        \"Search for recent developments in artificial intelligence\",\n",
        "    ]\n",
        "    for q in queries:\n",
        "        print(\"=\" * 50)\n",
        "        run_agent_query(q)\n",
        "        print()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "eBvoi2_7wHN_",
        "outputId": "bdb2ee1d-2a90-4250-d467-ac16f75b75bd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Retrieval-augmented generation (RAG) combines large language models (LLMs) with external knowledge sources, allowing the model to access and incorporate relevant information before generating a response.  This approach improves the accuracy, factual consistency, and overall quality of the generated text by grounding the LLM's output in specific evidence.\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"topic\"],\n",
        "    template=\"Write a two-sentence overview about {topic}:\"\n",
        ")\n",
        "\n",
        "chain = prompt | llm | StrOutputParser()\n",
        "print(chain.invoke({\"topic\": \"retrieval-augmented generation\"}))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "6OMDCwaCwLFA",
        "outputId": "a8983268-8107-4c70-d312-d1c406f2da97"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LangChain is a framework for developing applications powered by language models.\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "chat_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are a helpful assistant. Keep answers concise.\"),\n",
        "    (\"human\", \"{question}\")\n",
        "])\n",
        "\n",
        "print((chat_prompt | llm | StrOutputParser()).invoke({\"question\": \"What is LangChain?\"}))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "2acZPBUzwLqH",
        "outputId": "9fc26f42-4a43-4684-ddc3-428df266cdca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'title': 'Vector Databases', 'bullets': ['Store data as vectors, enabling similarity search based on semantic meaning rather than exact keyword matches.', 'Ideal for applications needing to find similar items, such as image search, recommendation systems, and anomaly detection.', 'Utilize techniques like approximate nearest neighbor (ANN) search to efficiently handle large datasets and high-dimensional vectors.']}\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.output_parsers import JsonOutputParser\n",
        "from pydantic import BaseModel, Field\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "\n",
        "class Note(BaseModel):\n",
        "    title: str = Field(..., description=\"Short title\")\n",
        "    bullets: list[str] = Field(..., description=\"3 concise bullet points\")\n",
        "\n",
        "parser = JsonOutputParser(pydantic_object=Note)\n",
        "\n",
        "tpl = PromptTemplate(\n",
        "    template=\"Create study notes as JSON about: {topic}\\n{format_instructions}\",\n",
        "    input_variables=[\"topic\"],\n",
        "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
        ")\n",
        "\n",
        "json_chain = tpl | llm | parser\n",
        "print(json_chain.invoke({\"topic\": \"vector databases\"}))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "LtwH1VgKwPLw",
        "outputId": "386d8889-fa73-4ccb-94f2-093c48562917"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "It's nice to meet you, Chandan!  How can I help you today?\n",
            "You said your name is Chandan.\n"
          ]
        }
      ],
      "source": [
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "\n",
        "memory = ConversationBufferMemory(return_messages=True)\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are friendly. Keep replies under 2 sentences.\"),\n",
        "    MessagesPlaceholder(\"history\"),\n",
        "    (\"human\", \"{input}\")\n",
        "])\n",
        "\n",
        "def chat_turn(text):\n",
        "    hist = memory.load_memory_variables({})[\"history\"]\n",
        "    out = (prompt | llm | StrOutputParser()).invoke({\"history\": hist, \"input\": text})\n",
        "    memory.save_context({\"input\": text}, {\"output\": out})\n",
        "    return out\n",
        "\n",
        "print(chat_turn(\"My name is Chandan.\"))\n",
        "print(chat_turn(\"What did I say my name is?\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "1tz-FKfqxhRM",
        "outputId": "a0c7fb6a-3467-459a-fedb-64cc51b7d387"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "FAISS enables fast similarity search for context retrieval in RAG.\n"
          ]
        }
      ],
      "source": [
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_core.documents import Document\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "docs = [\n",
        "    Document(page_content=\"LangChain provides prompts, chains, memory, and tools.\"),\n",
        "    Document(page_content=\"RAG retrieves context before generation for grounded answers.\"),\n",
        "    Document(page_content=\"FAISS enables fast vector similarity search.\"),\n",
        "]\n",
        "\n",
        "emb = GoogleGenerativeAIEmbeddings(model=\"gemini-embedding-001\")\n",
        "vs = FAISS.from_documents(docs, emb)\n",
        "retriever = vs.as_retriever(search_kwargs={\"k\": 2})\n",
        "\n",
        "rag_prompt = PromptTemplate.from_template(\n",
        "    \"Use the context to answer the question.\\n\\nContext:\\n{ctx}\\n\\nQuestion: {q}\\n\\nAnswer briefly:\"\n",
        ")\n",
        "\n",
        "def join(ds): return \"\\n\\n\".join(d.page_content for d in ds)\n",
        "\n",
        "rag_chain = ({\"ctx\": retriever | join, \"q\": RunnablePassthrough()} | rag_prompt | llm | StrOutputParser())\n",
        "print(rag_chain.invoke(\"What does FAISS do in RAG?\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "RXhks1iNxkMk",
        "outputId": "1a7db415-29bc-4994-d12a-036a5f8050b1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Retrieval-augmented generation (RAG) combines large language models (LLMs) with external knowledge sources.  It retrieves relevant information before generating text. This improves accuracy and reduces hallucinations.  The retrieved context is often included in the output.  RAG enhances LLM capabilities by grounding responses in factual data.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "chat_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"Be concise.\"),\n",
        "    (\"human\", \"Explain retrieval-augmented generation in 5 lines.\")\n",
        "])\n",
        "for chunk in (chat_prompt | llm | StrOutputParser()).stream({}):\n",
        "    print(chunk, end=\"\", flush=True)\n",
        "print()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "Fn1M6p_jxzGU",
        "outputId": "c4b4ec8c-3297-42d1-bd93-3b04c8f415b8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Retrieval Augmented Generation (RAG) uses a separate knowledge base to retrieve relevant information for context before generating a response, maintaining the model's general capabilities.  Fine-tuning, conversely, directly modifies the model's weights with task-specific data, improving performance on that task but potentially reducing its adaptability to other tasks.\n"
          ]
        }
      ],
      "source": [
        "validator = PromptTemplate.from_template(\n",
        "    \"Is the following text 2-3 sentences and formal tone? Answer yes/no.\\n\\n{text}\"\n",
        ")\n",
        "def ensure_constraints(topic):\n",
        "    text = chain.invoke({\"topic\": topic})\n",
        "    verdict = (validator | llm | StrOutputParser()).invoke({\"text\": text}).lower()\n",
        "    if \"yes\" in verdict:\n",
        "        return text\n",
        "    fix = PromptTemplate.from_template(\n",
        "        \"Rewrite to 2-3 sentences, formal tone:\\n\\n{text}\"\n",
        "    ) | llm | StrOutputParser()\n",
        "    return fix.invoke({\"text\": text})\n",
        "\n",
        "print(ensure_constraints(\"contrast RAG vs fine-tuning\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "FpSkSpkrx2e8",
        "outputId": "5adb5de2-6acd-4c7a-b066-c78a887bed1b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LangChain is a framework for developing applications that leverage the power of large language models (LLMs).  These applications are often enhanced through Retrieval Augmented Generation (RAG) systems, which improve LLM output by integrating information retrieved from external sources.  This retrieval process is frequently facilitated by vector databases, which efficiently store and search data based on semantic meaning via embeddings, enabling more accurate and contextually relevant responses.\n"
          ]
        }
      ],
      "source": [
        "docs = [\n",
        "    \"LangChain composes prompts, models, and tools.\",\n",
        "    \"RAG retrieves relevant chunks before generation.\",\n",
        "    \"Vector DBs enable semantic search over embeddings.\"\n",
        "]\n",
        "\n",
        "map_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"human\", \"Write a one-sentence summary:\\n\\n{context}\")\n",
        "])\n",
        "map_chain = map_prompt | llm | StrOutputParser()\n",
        "mapped = map_chain.batch([{\"context\": d} for d in docs])\n",
        "\n",
        "reduce_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"human\", \"Combine these summaries into one paragraph:\\n\\n{docs}\")\n",
        "])\n",
        "reduce_chain = reduce_prompt | llm | StrOutputParser()\n",
        "final_summary = reduce_chain.invoke({\"docs\": \"\\n\".join(f\"- {m}\" for m in mapped)})\n",
        "print(final_summary)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
