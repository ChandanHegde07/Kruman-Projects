{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## LangChain Fundamentals: Step-by-Step Practice with Prompts,Embeddings, Tools, RAG, and Agents Using Gemini 1.5 Flash\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "bUejK7rt2WP5",
        "outputId": "debccf31-7f42-47be-e855-75b06772434b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your Google AI API key: ··········\n"
          ]
        }
      ],
      "source": [
        "import os, getpass\n",
        "\n",
        "if \" \" not in os.environ:\n",
        "    os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter your Google AI API key: \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "81S0xHBr2_bX",
        "outputId": "5a7deff3-6d44-4370-c0f0-5f0f81cee57a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hello from LangChain!\n"
          ]
        }
      ],
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\")  # fast, inexpensive model\n",
        "resp = llm.invoke(\"Say hello from LangChain in one short sentence.\")\n",
        "print(resp.content)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "frUMf9iA4Gc5",
        "outputId": "6918d7ee-1aaf-47b0-d3eb-562bd8d1a0a4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Renewable energy sources offer a multitude of benefits compared to fossil fuels:\n",
            "\n",
            "**Environmental Benefits:**\n",
            "\n",
            "* **Reduced Greenhouse Gas Emissions:** This is arguably the most significant benefit.  Renewable energy sources produce little to no greenhouse gases during operation, significantly reducing our carbon footprint and mitigating climate change.\n",
            "* **Improved Air Quality:**  Burning fossil fuels releases harmful pollutants like particulate matter, sulfur dioxide, and nitrogen oxides, which contribute to respiratory illnesses and other health problems. Renewable energy sources drastically reduce these pollutants, leading to cleaner air.\n",
            "* **Reduced Water Pollution:** Fossil fuel extraction and processing often contaminate water sources. Renewable energy technologies generally have a much lower impact on water quality.\n",
            "* **Protection of Ecosystems:**  Renewable energy sources require less land disruption than fossil fuel extraction, helping to protect natural habitats and biodiversity.  While some land use is required for renewable energy projects, it's often less extensive and can sometimes coexist with other land uses (e.g., solar farms on grazing land).\n",
            "* **Reduced Waste:**  Renewable energy technologies generally produce less waste than fossil fuel power plants.\n",
            "\n",
            "\n",
            "**Economic Benefits:**\n",
            "\n",
            "* **Energy Independence:**  Renewable energy reduces reliance on imported fossil fuels, enhancing national energy security and reducing vulnerability to price volatility in global energy markets.\n",
            "{'input_tokens': 8, 'output_tokens': 256, 'total_tokens': 264, 'input_token_details': {'cache_read': 0}}\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-1.5-flash\",\n",
        "    temperature=0.7,\n",
        "    max_tokens=256,\n",
        ")\n",
        "\n",
        "# Simple usage\n",
        "response = llm.invoke(\"What are the benefits of renewable energy?\")\n",
        "print(response.content)        # Text output\n",
        "print(response.usage_metadata) # Optional: token usage details"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hdS1iplb4PR5",
        "outputId": "50db2fca-345e-4323-880d-46cd4a204052"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Imagine a light switch. It can be either ON or OFF, right?  That's how regular computers work – they use bits, which are either 0 or 1.\n",
            "\n",
            "Quantum computers are different. They use **qubits**.  A qubit can be 0, 1, or *both at the same time*! This \"both at the same time\" is called **superposition**.  It's like the light switch being both ON and OFF simultaneously – weird, but true.\n",
            "\n",
            "Another quantum trick is **entanglement**. Imagine two qubits linked together magically.  If you measure one and it's 0, you instantly know the other is 1, no matter how far apart they are!\n",
            "\n",
            "Because of superposition and entanglement, quantum computers can explore many possibilities at once.  Think of it like searching a maze: a regular computer tries each path one by one, while a quantum computer explores all paths simultaneously.  This makes them potentially much faster for certain types of problems.\n",
            "\n",
            "However, quantum computers aren't meant to replace regular computers. They're good at specific tasks, like:\n",
            "\n",
            "* **Drug discovery:** Simulating molecules to design new medicines.\n",
            "* **Materials science:** Designing new materials with specific properties\n"
          ]
        }
      ],
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_core.messages import HumanMessage, SystemMessage\n",
        "\n",
        "chat_model = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-1.5-flash\",\n",
        "    temperature=0.3,\n",
        "    max_tokens=256,\n",
        ")\n",
        "\n",
        "messages = [\n",
        "    SystemMessage(content=\"You are a helpful AI assistant.\"),\n",
        "    HumanMessage(content=\"Explain quantum computing in simple terms.\"),\n",
        "]\n",
        "\n",
        "response = chat_model.invoke(messages)\n",
        "\n",
        "print(response.content)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "jZVdtot94pR7",
        "outputId": "09b7f124-52ef-453d-d4b3-158382040301"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Embedding dimension: 3072\n",
            "First 5 values: [-0.011009139940142632, 0.003710047109052539, 0.007905430160462856, -0.07840708643198013, -0.012440552935004234]\n"
          ]
        }
      ],
      "source": [
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "\n",
        "embeddings = GoogleGenerativeAIEmbeddings(\n",
        "    model=\"gemini-embedding-001\"\n",
        ")\n",
        "\n",
        "text = \"LangChain is a powerful framework for LLM applications.\"\n",
        "embedding_vector = embeddings.embed_query(text)\n",
        "\n",
        "print(f\"Embedding dimension: {len(embedding_vector)}\")\n",
        "print(f\"First 5 values: {embedding_vector[:5]}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "SQAviZGF7dt1",
        "outputId": "1e97ea40-3722-436a-a29c-a55d713d6073"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "----- Formatted Prompt -----\n",
            "\n",
            "You are an expert financial planning consultant.\n",
            "Please provide advice on: retirement savings strategies\n",
            "Consider the following context: for a 30-year-old software engineer earning $120k annually\n",
            "Your advice:\n",
            "\n",
            "\n",
            "----- Model Response -----\n",
            "## Retirement Savings Strategies for a 30-Year-Old Software Engineer Earning $120k Annually\n",
            "\n",
            "Congratulations on your successful career as a software engineer!  At 30, you're in an excellent position to build a robust retirement plan.  However, simply saving is insufficient; a strategic approach is crucial.  Here's a comprehensive plan tailored to your situation:\n",
            "\n",
            "**I. Maximizing Employer-Sponsored Retirement Plans:**\n",
            "\n",
            "* **401(k) or Similar Plan:**  Your highest priority should be maximizing contributions to your employer-sponsored retirement plan.  If your employer offers a matching contribution (e.g., 50% up to 6% of your salary), contribute at least that much – this is essentially free money.  Aim to contribute the maximum allowable amount ($23,000 in 2023, plus an additional $7,500 catch-up contribution if you're 50 or older).  Consider increasing your contributions gradually over time to ease the adjustment.\n",
            "\n",
            "* **Understand Investment Options:** Don't just passively accept the default investment option.  Research the available funds within your plan.  A diversified portfolio of low-cost index funds (tracking the S&P 500, total stock market, and potentially bonds) is generally recommended for long-term growth.  Consider your risk tolerance and time horizon when making investment choices.  If you'\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.prompts import PromptTemplate\n",
        "\n",
        "template = \"\"\"\n",
        "You are an expert {expertise} consultant.\n",
        "Please provide advice on: {query}\n",
        "Consider the following context: {context}\n",
        "Your advice:\n",
        "\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"expertise\", \"query\", \"context\"],\n",
        "    template=template,\n",
        ")\n",
        "\n",
        "formatted_prompt = prompt.format(\n",
        "    expertise=\"financial planning\",\n",
        "    query=\"retirement savings strategies\",\n",
        "    context=\"for a 30-year-old software engineer earning $120k annually\",\n",
        ")\n",
        "print(\"----- Formatted Prompt -----\")\n",
        "print(formatted_prompt)\n",
        "\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-1.5-flash\",\n",
        "    temperature=0.3,\n",
        "    max_tokens=300,\n",
        ")\n",
        "\n",
        "response = llm.invoke(formatted_prompt)\n",
        "print(\"\\n----- Model Response -----\")\n",
        "print(response.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "Mf3Vsrb09xbr",
        "outputId": "0751a291-09a1-46e6-c290-ce50ad33668c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SystemMessage: You are a data scientist who specializes in machine learning model optimization.\n",
            "HumanMessage: How can I improve the performance of my neural network?\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.prompts.chat import (\n",
        "    SystemMessagePromptTemplate,\n",
        "    HumanMessagePromptTemplate,\n",
        ")\n",
        "\n",
        "# Create chat prompt template\n",
        "system_template = \"You are a {role} who {specialty}.\"\n",
        "system_message_prompt = SystemMessagePromptTemplate.from_template(system_template)\n",
        "\n",
        "human_template = \"{request}\"\n",
        "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
        "\n",
        "chat_prompt = ChatPromptTemplate.from_messages([\n",
        "    system_message_prompt,\n",
        "    human_message_prompt,\n",
        "])\n",
        "\n",
        "# Format the messages\n",
        "messages = chat_prompt.format_messages(\n",
        "    role=\"data scientist\",\n",
        "    specialty=\"specializes in machine learning model optimization\",\n",
        "    request=\"How can I improve the performance of my neural network?\"\n",
        ")\n",
        "\n",
        "for message in messages:\n",
        "    print(f\"{message.__class__.__name__}: {message.content}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "CV2VFzomnn7x",
        "outputId": "0b78e4c0-695f-407c-a64e-7a3ce7922c15"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You are a Python programming expert. Provide helpful library recommendations:\n",
            "\n",
            "Input: I need to analyze sales data\n",
            "Output: I recommend using pandas for data manipulation, matplotlib for visualization, and seaborn for statistical plots.\n",
            "\n",
            "Input: How do I build a web scraper?\n",
            "Output: For web scraping, use requests for HTTP, BeautifulSoup for HTML parsing, and optionally Selenium for dynamic pages.\n",
            "\n",
            "Input: I want to build a REST API for my project\n",
            "Output:\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.prompts import PromptTemplate, FewShotPromptTemplate\n",
        "\n",
        "# Define examples\n",
        "examples = [\n",
        "    {\n",
        "        \"input\": \"I need to analyze sales data\",\n",
        "        \"output\": \"I recommend using pandas for data manipulation, matplotlib for visualization, and seaborn for statistical plots.\"\n",
        "    },\n",
        "    {\n",
        "        \"input\": \"How do I build a web scraper?\",\n",
        "        \"output\": \"For web scraping, use requests for HTTP, BeautifulSoup for HTML parsing, and optionally Selenium for dynamic pages.\"\n",
        "    }\n",
        "]\n",
        "\n",
        "# Create example template\n",
        "example_template = \"\"\"\n",
        "Input: {input}\n",
        "Output: {output}\n",
        "\"\"\".strip()\n",
        "\n",
        "example_prompt = PromptTemplate(\n",
        "    input_variables=[\"input\", \"output\"],\n",
        "    template=example_template,\n",
        ")\n",
        "\n",
        "# Create few-shot prompt\n",
        "few_shot_prompt = FewShotPromptTemplate(\n",
        "    examples=examples,\n",
        "    example_prompt=example_prompt,\n",
        "    prefix=\"You are a Python programming expert. Provide helpful library recommendations:\",\n",
        "    suffix=\"Input: {query}\\nOutput:\",\n",
        "    input_variables=[\"query\"],\n",
        "    example_separator=\"\\n\\n\",\n",
        ")\n",
        "\n",
        "# Use the prompt\n",
        "formatted = few_shot_prompt.format(query=\"I want to build a REST API for my project\")\n",
        "print(formatted)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "LzSYzyNnn52-",
        "outputId": "3974b7a4-d11e-4709-8eed-3070b2e2a386"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Artificial intelligence (AI) is a broad field encompassing the development of computer systems capable of performing tasks that typically require human intelligence.  These tasks include learning, reasoning, problem-solving, perception, and natural language understanding.  AI systems achieve this through various techniques, including machine learning (where systems learn from data without explicit programming) and deep learning (a subset of machine learning using artificial neural networks with multiple layers).  AI is rapidly transforming many industries, from healthcare and finance to transportation and entertainment, offering both immense potential and significant ethical considerations.\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "# Create components\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"topic\"],\n",
        "    template=\"Write a brief summary about {topic}:\"\n",
        ")\n",
        "\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-1.5-flash\",\n",
        "    temperature=0.7,\n",
        ")\n",
        "\n",
        "# Create chain (prompt -> model -> string)\n",
        "chain = prompt | llm | StrOutputParser()\n",
        "\n",
        "# Run chain\n",
        "result = chain.invoke({\"topic\": \"artificial intelligence\"})\n",
        "print(result)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "9A2QtEv4owaW",
        "outputId": "8f6ac4ad-3255-4f4d-c6fb-2b3abf42b005"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.5/310.5 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hLoaded 2 pages\n",
            "Page 1: Here’s a demo setup diagram for your Smart Energy Optimization System:\n",
            "🔹 Demo Setup Diagram\n",
            "        ┌───────────────────────────┐\n",
            "        │        Sen...\n",
            "Page 2: This diagram is simple, clear , and examiner-friendly. ✅\n",
            "2...\n"
          ]
        }
      ],
      "source": [
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "\n",
        "# Load a PDF file\n",
        "loader = PyPDFLoader(\"Demo Setup Diagram.pdf\")\n",
        "pages = loader.load()\n",
        "\n",
        "print(f\"Loaded {len(pages)} pages\")\n",
        "for i, page in enumerate(pages[:3]):  # Show first 3 pages\n",
        "    print(f\"Page {i + 1}: {page.page_content[:150]}...\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "eBvoi2_7wHN_",
        "outputId": "bdb2ee1d-2a90-4250-d467-ac16f75b75bd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Retrieval-augmented generation (RAG) combines large language models (LLMs) with external knowledge sources, allowing the model to access and incorporate relevant information before generating a response.  This approach improves the accuracy, factual consistency, and overall quality of the generated text by grounding the LLM's output in specific evidence.\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"topic\"],\n",
        "    template=\"Write a two-sentence overview about {topic}:\"\n",
        ")\n",
        "\n",
        "chain = prompt | llm | StrOutputParser()\n",
        "print(chain.invoke({\"topic\": \"retrieval-augmented generation\"}))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "6OMDCwaCwLFA",
        "outputId": "a8983268-8107-4c70-d312-d1c406f2da97"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LangChain is a framework for developing applications powered by language models.\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "chat_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are a helpful assistant. Keep answers concise.\"),\n",
        "    (\"human\", \"{question}\")\n",
        "])\n",
        "\n",
        "print((chat_prompt | llm | StrOutputParser()).invoke({\"question\": \"What is LangChain?\"}))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "2acZPBUzwLqH",
        "outputId": "9fc26f42-4a43-4684-ddc3-428df266cdca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'title': 'Vector Databases', 'bullets': ['Store data as vectors, enabling similarity search based on semantic meaning rather than exact keyword matches.', 'Ideal for applications needing to find similar items, such as image search, recommendation systems, and anomaly detection.', 'Utilize techniques like approximate nearest neighbor (ANN) search to efficiently handle large datasets and high-dimensional vectors.']}\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.output_parsers import JsonOutputParser\n",
        "from pydantic import BaseModel, Field\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "\n",
        "class Note(BaseModel):\n",
        "    title: str = Field(..., description=\"Short title\")\n",
        "    bullets: list[str] = Field(..., description=\"3 concise bullet points\")\n",
        "\n",
        "parser = JsonOutputParser(pydantic_object=Note)\n",
        "\n",
        "tpl = PromptTemplate(\n",
        "    template=\"Create study notes as JSON about: {topic}\\n{format_instructions}\",\n",
        "    input_variables=[\"topic\"],\n",
        "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
        ")\n",
        "\n",
        "json_chain = tpl | llm | parser\n",
        "print(json_chain.invoke({\"topic\": \"vector databases\"}))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "LtwH1VgKwPLw",
        "outputId": "386d8889-fa73-4ccb-94f2-093c48562917"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "It's nice to meet you, Chandan!  How can I help you today?\n",
            "You said your name is Chandan.\n"
          ]
        }
      ],
      "source": [
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "\n",
        "memory = ConversationBufferMemory(return_messages=True)\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are friendly. Keep replies under 2 sentences.\"),\n",
        "    MessagesPlaceholder(\"history\"),\n",
        "    (\"human\", \"{input}\")\n",
        "])\n",
        "\n",
        "def chat_turn(text):\n",
        "    hist = memory.load_memory_variables({})[\"history\"]\n",
        "    out = (prompt | llm | StrOutputParser()).invoke({\"history\": hist, \"input\": text})\n",
        "    memory.save_context({\"input\": text}, {\"output\": out})\n",
        "    return out\n",
        "\n",
        "print(chat_turn(\"My name is Chandan.\"))\n",
        "print(chat_turn(\"What did I say my name is?\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "1tz-FKfqxhRM",
        "outputId": "a0c7fb6a-3467-459a-fedb-64cc51b7d387"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "FAISS enables fast similarity search for context retrieval in RAG.\n"
          ]
        }
      ],
      "source": [
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_core.documents import Document\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "docs = [\n",
        "    Document(page_content=\"LangChain provides prompts, chains, memory, and tools.\"),\n",
        "    Document(page_content=\"RAG retrieves context before generation for grounded answers.\"),\n",
        "    Document(page_content=\"FAISS enables fast vector similarity search.\"),\n",
        "]\n",
        "\n",
        "emb = GoogleGenerativeAIEmbeddings(model=\"gemini-embedding-001\")\n",
        "vs = FAISS.from_documents(docs, emb)\n",
        "retriever = vs.as_retriever(search_kwargs={\"k\": 2})\n",
        "\n",
        "rag_prompt = PromptTemplate.from_template(\n",
        "    \"Use the context to answer the question.\\n\\nContext:\\n{ctx}\\n\\nQuestion: {q}\\n\\nAnswer briefly:\"\n",
        ")\n",
        "\n",
        "def join(ds): return \"\\n\\n\".join(d.page_content for d in ds)\n",
        "\n",
        "rag_chain = ({\"ctx\": retriever | join, \"q\": RunnablePassthrough()} | rag_prompt | llm | StrOutputParser())\n",
        "print(rag_chain.invoke(\"What does FAISS do in RAG?\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "RXhks1iNxkMk",
        "outputId": "1a7db415-29bc-4994-d12a-036a5f8050b1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Retrieval-augmented generation (RAG) combines large language models (LLMs) with external knowledge sources.  It retrieves relevant information before generating text. This improves accuracy and reduces hallucinations.  The retrieved context is often included in the output.  RAG enhances LLM capabilities by grounding responses in factual data.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "chat_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"Be concise.\"),\n",
        "    (\"human\", \"Explain retrieval-augmented generation in 5 lines.\")\n",
        "])\n",
        "for chunk in (chat_prompt | llm | StrOutputParser()).stream({}):\n",
        "    print(chunk, end=\"\", flush=True)\n",
        "print()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "Fn1M6p_jxzGU",
        "outputId": "c4b4ec8c-3297-42d1-bd93-3b04c8f415b8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Retrieval Augmented Generation (RAG) uses a separate knowledge base to retrieve relevant information for context before generating a response, maintaining the model's general capabilities.  Fine-tuning, conversely, directly modifies the model's weights with task-specific data, improving performance on that task but potentially reducing its adaptability to other tasks.\n"
          ]
        }
      ],
      "source": [
        "validator = PromptTemplate.from_template(\n",
        "    \"Is the following text 2-3 sentences and formal tone? Answer yes/no.\\n\\n{text}\"\n",
        ")\n",
        "def ensure_constraints(topic):\n",
        "    text = chain.invoke({\"topic\": topic})\n",
        "    verdict = (validator | llm | StrOutputParser()).invoke({\"text\": text}).lower()\n",
        "    if \"yes\" in verdict:\n",
        "        return text\n",
        "    fix = PromptTemplate.from_template(\n",
        "        \"Rewrite to 2-3 sentences, formal tone:\\n\\n{text}\"\n",
        "    ) | llm | StrOutputParser()\n",
        "    return fix.invoke({\"text\": text})\n",
        "\n",
        "print(ensure_constraints(\"contrast RAG vs fine-tuning\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "FpSkSpkrx2e8",
        "outputId": "5adb5de2-6acd-4c7a-b066-c78a887bed1b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LangChain is a framework for developing applications that leverage the power of large language models (LLMs).  These applications are often enhanced through Retrieval Augmented Generation (RAG) systems, which improve LLM output by integrating information retrieved from external sources.  This retrieval process is frequently facilitated by vector databases, which efficiently store and search data based on semantic meaning via embeddings, enabling more accurate and contextually relevant responses.\n"
          ]
        }
      ],
      "source": [
        "docs = [\n",
        "    \"LangChain composes prompts, models, and tools.\",\n",
        "    \"RAG retrieves relevant chunks before generation.\",\n",
        "    \"Vector DBs enable semantic search over embeddings.\"\n",
        "]\n",
        "\n",
        "map_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"human\", \"Write a one-sentence summary:\\n\\n{context}\")\n",
        "])\n",
        "map_chain = map_prompt | llm | StrOutputParser()\n",
        "mapped = map_chain.batch([{\"context\": d} for d in docs])\n",
        "\n",
        "reduce_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"human\", \"Combine these summaries into one paragraph:\\n\\n{docs}\")\n",
        "])\n",
        "reduce_chain = reduce_prompt | llm | StrOutputParser()\n",
        "final_summary = reduce_chain.invoke({\"docs\": \"\\n\".join(f\"- {m}\" for m in mapped)})\n",
        "print(final_summary)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
